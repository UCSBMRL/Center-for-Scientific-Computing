{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yet another introduction to Neural Networks\n",
    "\n",
    "In this notebook, I will explain how to implement a neural network from scratch and use the version of MNIST dataset that is provided within Scikit-Learn for testing. I will specificallty illustrate the use of Python classes to define layers in the network as objects. Each layer object has forward and backward propagation methods which leads to compact, easily readable code. In writing this tutorial, I've had inspiration from [Peter Roelants'](http://peterroelants.github.io/) page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "After loading the data, we divide it into three parts, training, validation and testing sets. The validation set is to be used to determine the hyperparameters (i.e. number and size of hidden layers and regularization parameter) and the testing set is a separate picece of data to assess the final model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load the data and reshape images\n",
    "digits = load_digits()\n",
    "n_samples = len(digits['images'])\n",
    "data = digits['images'].reshape((n_samples, -1)); targets = digits['target']\n",
    "\n",
    "## Train-test splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, X_test, y, y_test = train_test_split(data, targets, test_size=0.33, random_state=111)\n",
    "\n",
    "## Train and validation splitting\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function and classifier\n",
    "\n",
    "We will use the sigmoid activation function $\\sigma(x) = 1/(1+e^{-x})$ in the layers of the network. For the classifier, we will use the softmax functio, which results in class probabilities:\n",
    "\n",
    "$$ {\\rm softmax}_i({\\bf Y}) = \\frac{e^{Y_i}}{\\sum_{k=1}^K\\, e^{Y_k}} $$\n",
    "\n",
    "where $Y_i$ is vector from the output of the neural network for a given example, and $K$ is the number of classes (10 in our case). Both functions are implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define the activation function, the Softmax classifier \n",
    "# Sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "# Softmax function\n",
    "def softmax(X):\n",
    "    temp = np.max(X, axis = 0) # Determine the maximum of each column\n",
    "    X = X - temp               # Subtract the max from each: does not change outcome\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above implementations, the assumption is that the arguments of both functions are cast in a matrix columnwise, so that each column represents one example: $ {\\bf X} \\in {\\mathbb R}^{n_{\\rm feat} \\times N}$ where $N$ is the number of examples and $n_{\\rm feat}$ is the number of features (i.e. number of pixels in our case). In softmax, we have subtracted the maximum of each column from each training example vector, an operation that does not change the outcome, for numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, the activation function and the classifier, we can construct the layers of the network. \n",
    "\n",
    "### Linear Update\n",
    "\n",
    "First, we define the class LinearUpdate which performs the linear transformation of the (derived) features in the current layer:\n",
    "\n",
    "$${\\bf Y}^{(i)} = {\\bf W}^{T} \\cdot A^{(i)} + {\\bf b}$$ \n",
    "\n",
    "In this equation, ${\\bf A}^{(i)} \\in {\\mathbb R}^{n_{\\rm in}} $ represents the **current layer state** (i.e features in case of input layer and derived features in case of hidden layers) with $n_{\\rm in}$ being the number of neurons. $Y^{(i)} \\in {\\mathbb R}^{n_{\\in out}}$ is the linear ouput of the current layer (which will later be the argument of an activation function) which is passed  as the **input** to the next layer with $n_{\\rm out}$ neurons. The supersript (i) refers to the training example being considered. Instead of using a *for loop* over the training examples, we can cast them in a matrix where each column is one training example vector $Y^{(i)}$, i.e.\n",
    "\n",
    "$$Y^{(i)}_j \\rightarrow Y_{ji} : {\\bf Y} \\in {\\mathbb R}^{n_{\\rm out} \\times N}$$\n",
    "\n",
    "where $N$ is the number of training examples. Similary $A^{(i)}_j \\rightarrow A_{ji} : {\\bf A} \\in {\\mathbb R}^{n_{\\rm in} \\times N}$.\n",
    "\n",
    "#### Forward propagation\n",
    "\n",
    "The above equation can be simply written in matrix notation as ${\\mathbf Y} = {\\mathbf W}^T \\cdot {\\bf A} + {\\bf b}$. The weights ${\\bf W} \\in {\\mathbb R}^{n_{\\rm in} \\times n_{\\rm out}}$ and biases ${\\bf b} \\in {\\mathbb R}^{n_{\\rm in}}$ will be determined during training by the minimization of a loss function $L$. \n",
    "\n",
    "The LinearUpdate object is initialized with $n_{\\rm in}$, $n_{\\rm out}$, the weights and biases. The forward method below implements the above equation. \n",
    "\n",
    "#### Back propagation\n",
    "\n",
    "The backpropagation method simply takes the gradient of the loss function with respect to the **state of the next layer** ($\\nabla_{\\bf Y} L$) and computes the gradients with respect to the **current state** ($\\nabla_{\\bf A}L$), **weights** ($\\nabla_{\\bf W}L$) and **biases** ($\\nabla_{\\bf b}L$). The backpropagation rules can be derived by noting that:\n",
    "\n",
    "$$ Y_{ji} = \\sum_{k=1}^{n_{\\rm in}}\\, \\left( W^T \\right)_{jk}\\, A_{ki} + b_j $$\n",
    "\n",
    "First, the gradient of the loss function $L$ with respect to the weights ${\\bf W}$ can be written as:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W_{ln}} = \\sum_{i=1}^N\\, \\sum_{j=1}^{n_{\\rm out}}\\, \\frac{\\partial L}{\\partial Y_{ji}}\\, \\frac{\\partial Y_{ji}}{\\partial W_{ln}} = \\sum_{i=1}^N\\, \\frac{\\partial L}{\\partial Y_{ni}}\\, A_{li}$$\n",
    "\n",
    "We denote the gradient from the next layer $\\frac{\\partial L}{\\partial Y_{ji}}$ as $\\nabla_{\\bf Y} L$, and the above equation in matrix form becomes\n",
    "\n",
    "$$ \\nabla_{\\bf W} L = {\\bf A} \\cdot (\\nabla_{\\bf Y} L)^T$$\n",
    "\n",
    "Similarly, we can compute gradients with respect to ${\\bf A}$ and ${\\bf b}$, and following similar steps, we obtain\n",
    "\n",
    "$$ ( \\nabla_{\\bf b} L )_l = \\sum_{i=1}^N\\, (\\nabla_{\\bf Y} L)_{li} $$\n",
    "$$ \\nabla_{\\bf A} L = {\\bf W} \\cdot (\\nabla_{\\bf Y} L) $$\n",
    "\n",
    "All of these three backpropagation rules are implemented below in the backward method of LinearUpdate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Linear Update\n",
    "class LinearUpdate(object):\n",
    "    def __init__ (self, n_in, n_out, W = None, b = None):\n",
    "        # Initialize W randomly\n",
    "        eps = np.sqrt(6.0) / np.sqrt(n_in + n_out)\n",
    "        self.W = (np.random.uniform(-eps, eps, (n_in, n_out)) if W is None else W)\n",
    "        \n",
    "        # Initialize biases as zero\n",
    "        self.b = np.zeros((n_out,1))\n",
    "        \n",
    "    def forward(self, A):\n",
    "        \"\"\" Forward propagation method: A is current state, method results in next state linearly\n",
    "            Y <- W.T * A + b\n",
    "        \"\"\"\n",
    "        return np.dot(self.W.T, A) + self.b\n",
    "    \n",
    "    def backward(self, A, gY):\n",
    "        \"\"\" Backward propagation method: A is current state, gY is backpropagated derivative from \n",
    "            next layer.\n",
    "        \"\"\"\n",
    "        gW = np.dot(A, gY.T)       # dL/dW_ab = sum_i A_ai * (gY)_bi\n",
    "        gA = np.dot(self.W, gY)    # dL/dA_ab = sum_j ( (W)_aj * gY_jb )  \n",
    "        gb = np.sum(gY, axis=1)    # dL/db_l = sum_i (gY)_li\n",
    "        \n",
    "        return gA, gW, gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Update\n",
    "\n",
    "Now we implement the object which takes the outcome of the linear update, and transforms it with the actiovation function. Our choice for the activation is the sigmoid function defined above. The activation function takes the **input into the layer** (let's denote by $Z$) and generates an output $\\sigma(Z)$ which will be passed to the next layer. Specifically, we will have:\n",
    "\n",
    "$$ {\\bf Z} = {\\bf W}^T \\cdot {\\bf A} + {\\bf b}$$\n",
    "$$ {\\bf Y} = \\sigma({\\bf Z})$$\n",
    "\n",
    "i.e., the linear output ${\\bf Z}$ will be transformed by the activation function and then passed to the next layer. \n",
    "\n",
    "The forward and backward propagation methods are easily obtained as follows:\n",
    "\n",
    "$$ {\\bf Y} = \\sigma(Z), \\quad\n",
    "\\frac{\\partial L}{\\partial {\\bf Z}} = \\frac{\\partial L}{\\partial {\\bf Y}} \\cdot \\frac{\\partial {\\bf Y}}{\\partial {\\bf Z}} = {\\bf Y} \\odot (1-{\\bf Y}) \\odot (\\nabla_{\\bf Y}L)$$\n",
    "\n",
    "where ${\\bf Y}$ is the **input into the next layer** and $\\nabla_{\\bf Y}L$ is the gradient with respect to the **input of the next layer** and $\\odot$ denote elementwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Logistic Update\n",
    "class LogisticUpdate(object):\n",
    "    def forward(self, Z):\n",
    "        \"\"\" Sigmoid activation: \"\"\"\n",
    "        return sigmoid(Z) #  Y = sigmoid(Z)\n",
    "    \n",
    "    def backward(self, Y, grad_out):\n",
    "        \"\"\" Backward propagation: \"\"\"\n",
    "        return np.multiply(Y * (1 - Y), grad_out)   # dL/dZ = dL/dY * Y * (1-Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "The output layer will be the classifier which deserves its own class. The forward method is still the sigmoid function, which will output class probabilities. The backward method implements the gradient of the loss function with respect to the outputs of the network. Finally, the crossEntropy method defines the loss function:\n",
    "\n",
    "$$ L = -\\frac{1}{N}\\, \\sum_{i=1}^{N}\\, \\sum_{k=1}^K\\, \\log(P_{ki})\\, I(T_i = k) $$\n",
    "\n",
    "where $P_{ki}$ is the calculated probability of training example $(i)$ being of class $k$, and the function $I$ is $1$ when the target (i.e. the actual value of the digit) is of class $k$, and zero otherwise. Evaluating the derivative of the loss function with respect to the **ouput layer state** ${\\bf Y}$ results in\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial {\\bf Y}}  = \\frac{1}{N}\\, ({\\bf P} - {\\bf I}) $$\n",
    "\n",
    "These methods are implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Softmax Classifier layer\n",
    "class SoftmaxClassifier(object):\n",
    "    def forward (self, A): \n",
    "        \"\"\" Given state A, produces output probs P by softmax \"\"\"\n",
    "        return softmax(A)\n",
    "    \n",
    "    def backward(self, P, T):\n",
    "        \"\"\" Given output probs P and targets T, produces output gradient \"\"\"\n",
    "        expansionMatrix = np.eye(P.shape[0], dtype = int)\n",
    "        expandedTarget = expansionMatrix[:, T]\n",
    "        \n",
    "        return P - expandedTarget # No division by number of samples yet.\n",
    "    \n",
    "    def crossEntropy(self, P, T):\n",
    "        \"\"\" Computes cross entropy \"\"\"\n",
    "        expansionMatrix = np.eye(P.shape[0], dtype = int)\n",
    "        expandedTarget = expansionMatrix[:, T]\n",
    "        \n",
    "        CE = -np.sum(expandedTarget * np.log(P + 1e-30))/P.shape[1]\n",
    "        return CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "Now that we have defined how states in a layer is forward propagated (first linearly, then by the activation function), we can use the LinearUpdate and LogisticUpdate classes to define the Layer class. The layer class first linearly transforms the current state vectors ${\\bf A}$ and then feeds them into the activation layer to yield the **input to the next layer** ${\\bf Y}$ by the forward method. In the backward method, the incoming gradient is first backpropagated through the logistic update, then by the linear update to yield the gradients with respect to **curent layer states**, weights and biases.\n",
    "\n",
    "Just for a sanity check, we can explicitly evaluate the backpropagation for ${\\bf A}$ as an example as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {\\bf Y} = \\sigma({\\bf Z}), \\quad {\\bf Z} = {\\bf W}^T \\cdot {\\bf A} + {\\bf b}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L}{\\partial A_{ln}} = \\sum_{i=1}^N\\, \\sum_{j=1}^{n_{\\rm out}}\\, \\frac{\\partial L}{\\partial Y_{ji}}\\, \\frac{\\partial Y_{ji}}{\\partial Z_{ji}}\\, \\frac{\\partial Z_{ji}}{\\partial A_{ln}} = \\sum_{j=1}^{n_{\\rm out}}\\, (\\nabla_{\\bf Y} L)_{jn}\\, Y_{jn}\\, (1-Y_{jn})\\, W_{lj}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ (\\nabla_{\\bf A} L) =  {\\bf W} \\cdot \\left[{\\bf Y} \\odot (1-{\\bf Y}) \\odot (\\nabla_{\\bf Y}L) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which are is implemented in two steps; first ${\\bf Y} \\odot (1-{\\bf Y}) \\odot (\\nabla_{\\bf Y}L)$ as the backward method of the LogisticUpdate which is fed into the backward method of LinearUpdate to yield its dot product with ${\\bf W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Layes (combine linear and logistic updates)\n",
    "class Layer(object):\n",
    "    def __init__ (self, n_in, n_out):\n",
    "        self.linear = LinearUpdate(n_in, n_out)\n",
    "        self.logistic = LogisticUpdate()\n",
    "        \n",
    "    def forward(self, A):\n",
    "        \"\"\" Forward propagation method \"\"\"\n",
    "        return self.logistic.forward(self.linear.forward(A))\n",
    "    \n",
    "    def backward(self, A, Y, grad_out):\n",
    "        \"\"\" Backward propagation method \"\"\"\n",
    "        # First the derivative of the logistic State\n",
    "        gZ = self.logistic.backward(Y, grad_out)\n",
    "        # Then, gZ becomes gY for the linear state\n",
    "        gA, gW, gb = self.linear.backward(A, gZ) \n",
    "        \n",
    "        return gA, gW, gb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the network\n",
    "\n",
    "Now we have the Layers and the Classifier objects, we can construct the neural network. We also add a regularization term to the loss function, which will be determined by the validation set performance below.\n",
    "The regularization term is given by\n",
    "\n",
    "$$ L_{\\rm reg} = \\frac{\\lambda}{2 N}\\, \\sum_{l=1}^{N_{\\rm layers}}\\, \\vert {\\bf W}_l \\vert^2 $$\n",
    "\n",
    "which is a sum over all the layers and penalizes the weights in all the layers using their square norm defined as\n",
    "\n",
    "$$ \\vert {\\bf W}_l \\vert^2 = \\sum_{i,j}\\, (W_l)_{ij}^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Construct a two hidden layer network\n",
    "class nnet(object):\n",
    "    def __init__ (self, nInput, nHidden1, nHidden2, K):\n",
    "        \"\"\" Initiate method for the net \"\"\"\n",
    "        self.inputLayer = Layer(nInput, nHidden1)     # Input layer\n",
    "        self.hiddenLayer1 = Layer(nHidden1, nHidden2) # 1st hidden layers\n",
    "        self.hiddenLayer2 = Layer(nHidden2, K)        # 2nd hidden layer\n",
    "        self.classifier = SoftmaxClassifier()         # Output: classification\n",
    "    \n",
    "    def forward(self, input_train):\n",
    "        \"\"\" Perform forward propagation through all layers \"\"\"\n",
    "        A1 = input_train.T                 # Initial data\n",
    "        A2 = self.inputLayer.forward(A1)   # inp -> hid1 \n",
    "        A3 = self.hiddenLayer1.forward(A2) # hid1 -> hid2\n",
    "        A4 = self.hiddenLayer2.forward(A3) # hid2 -> out\n",
    "        P = self.classifier.forward(A4)    # output probabilities\n",
    "        \n",
    "        return A1, A2, A3, A4, P\n",
    "    \n",
    "    def backward(self, P, A4, A3, A2, A1, T, lam = 0.0):\n",
    "        \"\"\" Back propagation method through all layers \"\"\"\n",
    "        grad_out = self.classifier.backward(P, T)                    # output grads\n",
    "        gA3, gW3, gb3 = self.hiddenLayer2.backward(A3, A4, grad_out) # hid2 grads\n",
    "        gA2, gW2, gb2 = self.hiddenLayer1.backward(A2, A3, gA3)      # hid1 grads\n",
    "        gA1, gW1, gb1 = self.inputLayer.backward(A1, A2, gA2)        # input grads\n",
    "        \n",
    "        # Add regularization terms to the gradients\n",
    "        if (lam > 0.0):\n",
    "            gW3 += lam * self.hiddenLayer2.linear.W\n",
    "            gW2 += lam * self.hiddenLayer1.linear.W\n",
    "            gW1 += lam * self.inputLayer.linear.W\n",
    "        \n",
    "        return grad_out, gW3, gb3, gW2, gb2, gW1, gb1\n",
    "    \n",
    "    def Loss(self, P, T, lam = 0.0):\n",
    "        \"\"\" Method for computing loss \"\"\"        \n",
    "        # Regularization term\n",
    "        reg = np.sum(self.inputLayer.linear.W**2) + np.sum(self.hiddenLayer1.linear.W**2) + \\\n",
    "              np.sum(self.hiddenLayer2.linear.W**2)\n",
    "        \n",
    "        # Full loss\n",
    "        return self.classifier.crossEntropy(P, T) + (0.5*lam/P.shape[1]) * reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function that will be used to train the network. We use the momentum method to update the gradients of weights as follows:\n",
    "\n",
    "$$ {\\bf V} \\leftarrow \\mu\\, {\\bf V} - \\alpha\\, \\nabla_{\\bf W}\\, L $$\n",
    "$$ {\\bf W} \\leftarrow {\\bf W} + {\\bf V} $$\n",
    "\n",
    "where $\\mu$ is the momentum and $\\alpha$ is the learning rate. A similar update method also holds for the biases. All velocities are initialized as zeros in the beginning. You can read more about the mometum method [here](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum). \n",
    "\n",
    "This function returns the trained network, calculated training and validation losses, and the validation accuracy. As a side note, we are using all the training data to compute the gradients. This is because the training set is small enough. In real situtations, one would be advised to batches. You can read more about these [here](http://sebastianruder.com/optimizing-gradient-descent/index.html#batchgradientdescent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNetwork(epochs, learningRate, momentum, lam, n_hidden_1, n_hidden_2, K):\n",
    "    \"\"\" Function for training the network \"\"\"\n",
    "    \n",
    "    ## Construct network\n",
    "    network = nnet(n_input, n_hidden_1, n_hidden_2, K)\n",
    "    \n",
    "    ## Initiate velocities\n",
    "    v_W1 = np.zeros_like(network.inputLayer.linear.W) \n",
    "    v_W2 = np.zeros_like(network.hiddenLayer1.linear.W)\n",
    "    v_W3 = np.zeros_like(network.hiddenLayer2.linear.W)\n",
    "    v_b1 = np.zeros_like(network.inputLayer.linear.b) \n",
    "    v_b2 = np.zeros_like(network.hiddenLayer1.linear.b)\n",
    "    v_b3 = np.zeros_like(network.hiddenLayer2.linear.b)\n",
    "    \n",
    "    ## Train\n",
    "    losses = {'train':[], 'validation':[]}\n",
    "    for epoch in range(epochs):\n",
    "        # Feed forward \n",
    "        A1, A2, A3, A4, P = network.forward(X_train)\n",
    "        \n",
    "        # Backprop\n",
    "        grad_out, gW3, gb3, gW2, gb2, gW1, gb1 = network.backward(P, A4, A3, A2, A1, y_train, lam)\n",
    "    \n",
    "        # Update weights by momentum method (Divide by the number of examples in the batch)\n",
    "        v_W1 = momentum * v_W1 - learningRate * gW1/n_train\n",
    "        v_W2 = momentum * v_W2 - learningRate * gW2/n_train\n",
    "        v_W3 = momentum * v_W3 - learningRate * gW3/n_train\n",
    "    \n",
    "        network.hiddenLayer2.linear.W += v_W3\n",
    "        network.hiddenLayer1.linear.W += v_W2\n",
    "        network.inputLayer.linear.W += v_W1\n",
    "    \n",
    "        # Update biases by momentum method (Divide by the number of examples in the batch)\n",
    "        v_b1 = momentum * v_b1 - learningRate * gb1[:,None]/n_train\n",
    "        v_b2 = momentum * v_b2 - learningRate * gb2[:,None]/n_train\n",
    "        v_b3 = momentum * v_b3 - learningRate * gb3[:,None]/n_train\n",
    "    \n",
    "        network.hiddenLayer2.linear.b += v_b3\n",
    "        network.hiddenLayer1.linear.b += v_b2\n",
    "        network.inputLayer.linear.b += v_b1\n",
    "    \n",
    "        # Compute loss on training set\n",
    "        loss_train = network.Loss(P, y_train, lam)\n",
    "        losses['train'].append(loss_train)\n",
    "    \n",
    "        # Compute loss on validation set\n",
    "        A1, A2, A3, A4, P = network.forward(X_valid)\n",
    "        loss_valid = network.Loss(P, y_valid, lam)\n",
    "        losses['validation'].append(loss_valid)\n",
    "        \n",
    "        # Compute validation accuracy\n",
    "        pred = np.argmax(P, axis = 0)\n",
    "        \n",
    "        # Accuracy\n",
    "        acc = np.sum((pred == y_valid).astype(float))/y_valid.shape[0]\n",
    "\n",
    "    # Return the trained network and the losses\n",
    "    return network, losses, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the hyperparameters\n",
    "\n",
    "Now we will train the network for a range of values for hyperparameters and determine the best ones based on their performance on the validation set. For illustration, we only choose to optimize the regularization parameter $\\lambda$ and keep all others constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 1 of 10 networks\n",
      "Trained 2 of 10 networks\n",
      "Trained 3 of 10 networks\n",
      "Trained 4 of 10 networks\n",
      "Trained 5 of 10 networks\n",
      "Trained 6 of 10 networks\n",
      "Trained 7 of 10 networks\n",
      "Trained 8 of 10 networks\n",
      "Trained 9 of 10 networks\n",
      "Trained 10 of 10 networks\n"
     ]
    }
   ],
   "source": [
    "## Setup: For this example, just vary the regularization parameter\n",
    "epochs=1000\n",
    "learningRate = 0.2\n",
    "momentum = 0.9\n",
    "\n",
    "# Network structure\n",
    "n_hidden_1 = 48\n",
    "n_hidden_2 = 32\n",
    "K = 10\n",
    "n_input = X_train.shape[1]\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# Values of lambda to try\n",
    "lambda_l2 = np.logspace(-2,2,10)\n",
    "\n",
    "# Train\n",
    "dict_acc = {'lambda':[], 'acc':[]}; count=1\n",
    "for lam in lambda_l2:\n",
    "    network, _, acc = trainNetwork(epochs, learningRate, momentum, lam, n_hidden_1, n_hidden_2, K)\n",
    "    dict_acc['lambda'].append(lam)\n",
    "    dict_acc['acc'].append(acc)\n",
    "    print \"Trained %d of %d networks\" % (count, lambda_l2.shape[0])\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the accuracy as a function of the regularization parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7efc663c6a90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAF5CAYAAAA/GEgLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xl4lOW9//F3AEHBBSqQoAWP6ynqaTWRU/etitZdq9VU\njxWrFTeCtlV7Wpcee47n9LjvetCKVePP1lrUuuBSLdalmuC+1KrgggIitIgUBfL74zu5GMeEJDPP\nzDOTeb+uK03mmWcyXx9o+OR+7vt7gyRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiSp\nm3YE7gLeA5YD+3fjNTsBLcBi4A3guKJVJ0mSEtUn7QKAgcB04MTM47Yuzl8fuAd4FNgC+C/gUuCg\nYhUoSZJ6r+XAfl2c8z/ASznHrgIeL0pFkiQpUeUw8tFT2wBTc45NBbYC+pa+HEmS1BOVGD5qgdk5\nx2YD/YChpS9HkiT1RL+0CyiREZkPSZLUM+9nPhJTieHjA6Au51gtsBT4sIPzR6yzzjqzZs2aVfTC\nJEnqhd4DxpBgAKnE8PEEsG/OsbHA08CyDs4fMWvWLG666SZGjx5d9OIUJk6cyMUXX5x2GVXFa156\nXvPS85qX1iuvvMIRRxyxLnH3oFeFj0HAxlmPNyCW0M4D3gHOA9YBvpt5/mrgJOACYBIxAfVo4LCV\nvcno0aOpr69PtHB1bvDgwV7vEvOal57XvPS85r1DOYSPMcDDma/bgAszX99AhIo6YGTW+TOAvYCL\niN4g7wEnA3cUv1RJklSocggfj7DyVTfjOjj2R6ChKNVIkqSiqsSltpIkqYIZPlQUjY2NaZdQdbzm\npec1Lz2vee9Qk3YBJVAPtLS0tDhJSZKkHmhtbaWhoQFiqkNrUt+3akY+2tpg8mSYOTPtSiRJqm5V\nEz5mzICjjoKddoK33067GkmSqlfVhI9HHoGBA6GmBr7xDbDhqSRJ6aia8PHoo7DHHvDww7B4Mey2\nG8yZk3ZVkiRVn6oJHy+8APvvD+uvHwHko49g993jsyRJKp2qCR8Ae+8dnzfZBB56KG69jB0LCxem\nW5ckSdWkasJHTQ0MHbri8WabwdSp0NICd96ZXl2SJFWbqgkf1177xWNbbglrrunkU0mSSqlqwkdn\n/cVqa2H27NLWIklSNaua8NGZujr44IO0q5AkqXpUffiorTV8SJJUSlUfPurqvO0iSVIpGT687SJJ\nUklVffiorYV58+Czz9KuRJKk6lD14aOuLna8nTs37UokSaoOho+6+OytF0mSSqPqw0dtbXx20qkk\nSaVR9eFj+PD47MiHJEmlUfXhY8AAGDLE8CFJUqlUffgAe31IklRKhg/s9SFJUikZPnBzOUmSSsnw\ngSMfkiSVkuEDw4ckSaVk+CBuuyxYAEuWpF2JJEm9n+GDFV1OnfchSVLxGT6wy6kkSaVk+MD9XSRJ\nKiXDBzBsGNTUGD4kSSoFwwfQrx8MHeptF0mSSsHwkeFyW0mSSsPwkWH4kCSpNAwfGbZYlySpNAwf\nGY58SJJUGoaPDEc+JEkqDcNHRl0dLFwIixalXYkkSb2b4SPDFuuSJJWG4SPDFuuSJJWG4SPDFuuS\nJJWG4SNj7bWhb1/DhyRJxWb4yOjTB4YP97aLJEnFZvjIYq8PSZKKz/CRxV4fkiQVn+EjiyMfkiQV\nn+Eji+FDkqTiM3xkab/t0taWdiWSJPVeho8sdXWweHG0WZckScVh+MjS3uXUWy+SJBVPuYSPE4C3\ngMXAM8D2XZx/JPA8sAiYBVwPfKnQItzfRZKk4iuH8HEocBFwLrAFMA24FxjZyfk7E2HjWmBT4BBg\nDDCp0EJssS5JUvGVQ/g4lQgO1wOvAacA7wDHd3L+VsAM4HJgJvAnIohsVWghgwdD//6OfEiSVExp\nh4/+QD0wNef4VGDbTl4zFagFvgnUZL4+BLi70GJqamLehyMfkiQVT9rhYyjQF8gda5gD1HXymueJ\nOR+/BpYA7wPzgAlJFGSvD0mSiivt8JGPrYEbgLOJUZM9gQ2Aq5P45rZYlySpuPql/P4fAsuIWyfZ\naokRjY6cAtwPXJB5/CKx6mUa8BO+OIoCwMSJExk8ePDnjjU2NtLY2Pi5Y3V18Oyz3f8PkCSpN2hu\nbqa5uflzxxYsWFCU90o7fHwKtABjgSlZx3cH7ujkNTVEYMm2POu5Dl188cXU19d3WZAjH5KkatTR\nL+Stra00NDQk/l5phw+AC4FfEf09ngS+D3yZFbdRzgPWAb6befw74rbLeGLy6QjgYuApoODZGu1z\nPtraYgKqJElKVjmEj9uAtYGziCDxArAXsdwWYuJpds+PW4C1gJOIWy8LgIeA05Mopq4OPvsM5s+H\nLxXctkySJOUqh/ABcFXmoyPjenh+QdpbrM+ebfiQJKkYKnG1S1HZ5VSSpOIyfOQwfEiSVFyGjxyr\nrw6rreaKF0mSisXwkaOmxi6nkiQVk+GjA/b6kCSpeAwfHXDkQ5Kk4jF8dMDwIUlS8Rg+OuBtF0mS\nisfw0YG6OpgzB5bl7iAjSZIKZvjoQG1tBI9589KuRJKk3sfw0YH2RmPeepEkKXmGjw7Y5VSSpOIx\nfHQge3M5SZKULMNHBwYOhDXWcORDkqRiMHx0wl4fkiQVh+GjE/b6kCSpOAwfnXDkQ5Kk4jB8dMKR\nD0mSisPw0QlHPiRJKg7DRyfq6uDDD2Hp0rQrkSSpdzF8dKK2FtraYO7ctCuRJKl3MXx0wi6nkiQV\nh+GjE4YPSZKKw/DRieHD47MrXiRJSpbhoxMDBsCQIY58SJKUNMPHStjrQ5Kk5Bk+VsJeH5IkJc/w\nsRKGD0mSkmf4WAlvu0iSlDzDx0o48iFJUvIMHytRVwfz58OSJWlXIklS72H4WIna2vg8Z066dUiS\n1JsYPlbCLqeSJCXP8LES7SMfTjqVJCk5ho+VGDYMamoc+ZAkKUmGj5VYZRUYOtTwIUlSkgwfXbDX\nhyRJyTJ8dMFeH5IkJcvw0QXDhyRJyTJ8dMHbLpIkJcvw0QVHPiRJSpbhowu1tbBwIXzySdqVSJLU\nOxg+utDe5dRbL5IkJcPw0QVbrEuSlCzDRxdssS5JUrIMH11Ye23o29eRD0mSkmL46ELfvrHHiyMf\nkiQlw/DRDS63lSQpOYaPbjB8SJKUHMNHN9jlVJKk5Bg+usGRD0mSklMu4eME4C1gMfAMsH0X5w8A\n/hOYAfwD+CswrljFtYePtrZivYMkSdWjX9oFAIcCFwHHA38CxgP3ApsC73TymtuAYcDRRPAYDqxS\nrAJra2HxYvj4Y1hjjWK9iyRJ1aEcwsepwCTg+szjU4A9iDDy7x2cvyewI7A+sCBz7O1iFpjd5dTw\nIUlSYdK+7dIfqAem5hyfCmzbyWv2I27NnAG8C7wG/C+wapFqtMupJEkJSnvkYyjQF8j9Z30OUNfJ\nazYg5oQsBg4gbr9cCaxN3IZJnPu7SJKUnLTDRz76AMuBw4GFmWOnAr8hbtUs6ehFEydOZPDgwZ87\n1tjYSGNjY5dvOGQIrLKK4UOS1Hs1NzfT3Nz8uWMLFizo5OzCpB0+PgSWAbU5x2uB9zt5zfvALFYE\nD4BXgRrgy8AbHb3o4osvpr6+Pq8ia2rs9SFJ6t06+oW8tbWVhoaGxN8rnzkfM4CzgVEJvP+nQAsw\nNuf47sDjnbzmMWAdYFDWsU2I0ZB3E6ipQ/b6kCQpGfmEjwuA/Ym+HA8AjUTfjXxdCBxD9OkYTSy7\n/TJwdeb584DJWeffAswDfpk5f0diwul1dHLLJQmGD0mSkpFP+LiMWKFSD7ycefwBcAWQz9jMbcBE\n4CxgOjGZdC9W9PioA0Zmnb+IGBkZTKx6uQmYAkzI4727zdsukiQlo5Clts8BTcAI4GfA94CngWcz\nX9f04HtdRfTtWBUYQ9xaaTcO2DXn/NeIWzWDiNs/P6KIox7gyIckSUkpZMJpf+BAIhzsBjxJNAob\nAfw8c6zrpSQVon3ko60tJqBKkqT85BM+GojA0UisVLmRuG3yatY5U4hbIr1GXR18+iksWBBLbyVJ\nUn7yCR9PExNNxxMh49MOznkLuLWAuspOdqMxw4ckSfnLJ3xsQCy3XZlFwFF5fO+yld1iffTodGuR\nJKmS5TPhdDjw9Q6Obw1sVVg55csW65IkJSOf8HEFsG4Hx9fNPNcrrbEGrLaa4UOSpELlEz5GE/04\nck0HNiusnPJli3VJkpKRT/hYQiynzVUHLC2snPJmrw9JkgqXT/h4APgvosNouyFEG/QHkiiqXDny\nIUlS4fIJHz8k2p3PBP4APEIsra0FfpBYZWXIkQ9JkgqXz1Lbd4GvAd8BtgAWE51Nm4HPkiut/Bg+\nJEkqXL7t1T8Grk2ykEpQWwtz5sDy5dCnkF1xJEmqYvmGjxpi1csoYo+XbHcWVFEZq6uDZctg3jwY\nNiztaiRJqkz5dji9A/iXTp7vtWMC2V1ODR+SJOUnn6BwCdFevZZoo745sCOxkdzOSRVWjuxyKklS\n4fIZ+dgG2BWYCywndrZ9DDiDCCZbJlZdmWkf+TB8SJKUv3xGPvoSE04BPgTWyXz9NvCVJIoqV4MG\nweqr2+tDkqRC5DPy8RLwVeBN4CngNOBT4LjMsV7N5baSJBUmn/Dxc2Bg5uszgbuAacA84LCE6ipb\nhg9JkgqTT/i4L+vrN4BNgbWB+cQckF7NFuuSJBWmp3M++hObx22ec3weVRA8wJEPSZIK1dPw8Skx\nsbRvEWqpCI58SJJUmHxWu/yc2MF27YRrqQh1dTB3LixdmnYlkiRVpnzmfJwMbATMIna2XZT1XBtQ\nn0BdZauuDtraIoCMGJF2NZIkVZ58wseUlTzXlm8hlSK7xbrhQ5KknssnfJyTdBGVxBbrkiQVptdu\nAlcstliXJKkw+Yx8rGxJbRu9fCXMgAEweLArXiRJylc+4eOgnMerAFsA36VKbsnY60OSpPzlEz5+\n18GxXxN7vhwKTCqoogpgrw9JkvKX5JyPPwO7Jfj9ypYjH5Ik5S+p8DEQOAl4L6HvV9YMH5Ik5S+f\n2y7zcx7XAGsAnwBHFFxRBfC2iyRJ+csnfJyS83g5MBd4Cvio4IoqQF0dfPQRLFkSq18kSVL35RM+\nbki6iErT3mhszhwYOTLdWiRJqjT5zPk4Gjikg+OHEMtte71NN4WaGpg6Ne1KJEmqPPmEjx8Dczo4\nPhf498LKqQzrrQd77w2XXhqbzEmSpO7LJ3yMJHazzTUTWK+wcipHUxM8/zw8+mjalUiSVFnyCR9z\ngK91cPyrwLzCyqkc3/gGbLYZXHJJ2pVIklRZ8gkftwKXArsS+7j0Bb6ROXZrcqWVt5oamDABpkyB\nt95KuxpJkipHPuHjTOBJ4EHgH5mPqcBDVMmcj3ZHHBGbzF1+edqVSJJUOfIJH0uIPVy+AhxObDS3\nIbEKZklypZW/gQPh2GPhuuvg44/TrkaSpMpQSHv1vwC3AXcBMxKppgKdeGIEj8mT065EkqTKkE/4\n+C3wow6On0bsbltVRo2CAw+MZbfLl6ddjSRJ5S+f8LEDcG8Hx+8FdiqsnMrU1AR/+Qvcf3/alUiS\nVP7yCR+rA591cHwpsGZh5VSm7baD+nqX3UqS1B35hI+XgMM6OH4o8HJh5VSmmpoY/bj/fnj11bSr\nkSSpvOUTPv4D+ClwI7GXy3eBX2WOnZtcaZXl0EOhtjbmfkiSpM7lEz7uBA4ANgKuBC4A1iUajVVt\nu60BA2D8+Fj1Mn9+2tVIklS+8l1q+3tgW2AQEUJuBy4CWhKqqyKNHw+ffRZ9PyRJUscK6fPxDeAW\n4D3gZOAeYKskiqpUdXVw2GHR8XTp0rSrkSSpPPU0fHyZaK/+JjHa8XdgAPAtYs7H9DzrOIG4ZbMY\neAbYvpuv245YZZPv+yauqQlmzoQ770y7EkmSylNPwsd9RCfTPYCfASOA8UBb5iNfhxK3bM4FtgCm\nET1DRnbxusHEpNcHC3z/RDU0xNJbl91KktSxnoSPscScjrOAycQoRRJOBSYB1wOvAacA7wDHd/G6\nq4GbgCeAmoRqSURTE/zxj/Dss2lXIklS+elJ+NgOeAH4HfA68GNi9KMQ/YF6YlfcbFOJCa2dGQf8\nEzECU1bBA6Ld+siRjn5IktSRnoSPJ4BjgHWA84B9gJlAX2JUZI083n9o5vWzc47PAeo6ec3Gmfc/\nAijL3VT69YsN5265BebMSbsaSZLKSz6rXT4mbpFsB/wLcD5wBjCX2OG2mPoSK2zOBv5a5PcqyLHH\nQt++cM01aVciSVJ5SeqWRT9iJORoYL8evK4/sAg4GJiSdfwS4KvALjnnDwY+ApZlHetD/HcsA3YH\nHsl5TT3QssMOOzB48ODPPdHY2EhjY2MPyu2Z446LVS8zZ0L//kV7G0mSCtbc3Exzc/Pnji1YsIBp\n06YBNACtSb1XOcyXeJKYyHpi1rGXgTuAn+ScWwOMzjl2IrArsdx3BvBJzvP1QEtLSwv19fUJldw9\nL70Em28ON90Ehx9e0reWJKlgra2tNDQ0QMLho5AmY0m5kJhLMo4IFhcR/USuzjx/HrG6BmJJ7cs5\nH3OBf2S+zg0eqdpsM9htt5h42lY2i4ElSUpXOYSP24CJxBLe6USDsb2I5bYQE09X1vOj0D4jRdXU\nBE8/DU8+mXYlkiSVh3IIHwBXAesDqwJjgMeynhtH3FbpzM+IWytlaa+9YKONXHYrSVK7cgkfvVaf\nPnDyyfCb38C776ZdjSRJ6TN8lMBRR8HAgXDllWlXIklS+gwfJbDmmnD00XDttbA4qab0kiRVKMNH\niZx8Mnz0Edx8c9qVSJKULsNHiWy4Ieyzj8tuJUkyfJRQUxO8+CL84Q9pVyJJUnoMHyW0667ReOzS\nS9OuRJKk9Bg+SqimBiZMiP1e3nwz7WokSUqH4aPEjjgChgyByy9PuxJJktJh+CixgQPh2GPhuutg\n4cK0q5EkqfQMHyk48URYtAgmT+76XEmSehvDRwpGjoSDDoqJp8uXp12NJEmlZfhISVMTvP463Hdf\n2pVIklRaho+UbLstNDS4260kqfoYPlJSUxOjH1OnwiuvpF2NJEmlY/hI0be/DbW1Nh2TJFUXw0eK\nBgyA44+HG2+E+fPTrkaSpNIwfKRs/HhYuhQmTUq7EkmSSsPwkbLaWjjssOh4unRp2tVIklR8ho8y\n0NQEb78NU6akXYkkScVn+CgD9fWw/fYuu5UkVQfDR5loaoJp02D69LQrkSSpuAwfZeKAA2DUKEc/\nJEm9n+GjTPTrFxvONTfD7NlpVyNJUvEYPsrIMcdA375wzTVpVyJJUvEYPsrIl74ERx4JV10Fn36a\ndjWSJBWH4aPMTJgAH3wAt92WdiWSJBWH4aPMbLop7L57TDxta0u7GkmSkmf4KENNTfDMM/DEE2lX\nIklS8gwfZeib34SNN3bZrSSpdzJ8lKE+feDkk+H22+Gdd9KuRpKkZBk+ytRRR8GgQXDllWlXIklS\nsgwfZWqNNeDoo+Haa+GTT9KuRpKk5Bg+ytjJJ8P8+XDzzWlXIklScgwfZWyDDWDffV12K0nqXQwf\nZa6pCV56CR5+OO1KJElKhuGjzO2yC2y+uctuJUm9h+GjzNXURMv1u++GN95IuxpJkgpn+KgAhx8O\nQ4bA5ZenXYkkSYUzfFSAgQPh+9+H66+HhQvTrkaSpMIYPirECSfAokVwww1pVyJJUmEMHxVi5Ej4\n1rfgsstg+fK0q5EkKX+GjwrS1ASvvw733pt2JZIk5c/wUUG22Qa22splt5Kkymb4qCA1NTH68cAD\n8PLLaVcjSVJ+DB8V5tvfhro6uPTStCuRJCk/ho8K078/HH883HgjfPRR2tVIktRzho8KdNxxsGwZ\nTJqUdiWSJPWc4aMC1dZCY2N0PF26NO1qJEnqGcNHhWpqgnfegd/9Lu1KJEnqGcNHhdpyS9hhB5fd\nSpIqTzmFjxOAt4DFwDPA9is59yDgAWAO8DfgcWBssQssN01N8Nhj0NqadiWSJHVfuYSPQ4GLgHOB\nLYBpwL3AyE7O3wG4H/gmUA88DNyVeW3V2H9/GDXK0Q9JUmUpl/BxKjAJuB54DTgFeAc4vpPzTwHO\nB1qAN4CfAq8D+xa90jLSrx+cdBLceivMnp12NZIkdU85hI/+xOjF1JzjU4Ftu/k9+gBrAPMSrKsi\nHHNMhJCrr067EkmSuqccwsdQoC+Q+7v7HKCum9/jB8BA4LYE66oIQ4bAkUfCVVfBkiVpVyNJUtfK\nIXwUqhE4m5g38mHKtaRiwoS47XJb1UUvSVIl6pd2AURgWAbU5hyvBd7v4rWHEnNFDiYmnXZq4sSJ\nDB48+HPHGhsbaWxs7FGx5Wj0aBg7NiaeHnFEbEAnSVJPNDc309zc/LljCxYsKMp7lcs/U08Sk0dP\nzDr2MnAH8JNOXtMIXEcEkLtW8r3rgZaWlhbq6+sTKLU83XMP7L13LL3dbru0q5Ek9Qatra00NDQA\nNACJNXYol9suFwLHAOOA0cSy2y8D7dMozwMmZ53/HeBGYq7H08TckDpgzRLVW3b23BM22cRlt5Kk\n8lcu4eM2YCJwFjCdaDC2F7HcFiJYZPf8OJao/QpgVtbHxSWqt+z06QMnnwy//W20XZckqVyVS/gA\nuApYH1gVGAM8lvXcOGDXrMe7ECtk+uR8HF2SSsvUd78LgwbF6pfbb4dFi9KuSJKkLyqn8KECrbEG\nXHcdzJ8PBx8Mw4bBt74Ft9wCf/972tVJkhQMH73MwQfDs8/CX/8K55wD774Lhx8eQWSffeCXv4R5\nVdeKTZJUTgwfvdSGG8Jpp8FTT8Hbb8MvfhGjH9/7HtTWwu67R1fUDz5Iu1JJUrUxfFSBkSNjB9w/\n/hFmzYLLL4/jJ50E66wDO+4Yq2ScqCpJKgXDR5Wpq4Px4+GBB6Ir6nXXwZprxijJqFHw9a/HKMkb\nb6RdqSSptzJ8VLG114Zx4+Duu2HOHLj5Zlh33ZgrstFGsMUWcO658PLLaVcqSepNDB8CYK214Dvf\niT4hc+fCr38dbdt/8QvYbLP4+qc/henToa0t7WolSZXM8KEvGDQoVs00N0cQufPOuB1z5ZVQXx+j\nIj/6UUxmXb487WolSZXG8KGVWnVV2HdfuOGGmCNy//2w225w442w9daw3norJrMuW5Z2tZKkSmD4\nULetskrsnnvNNbFq5pFH4MADo5vqTjvFypn2yayffZZ2tZKkcmX4UF769o3Aceml0UfkiSeirfvU\nqRFQ6upWTGZdsiTtaiVJ5cTwoYL16RO3YP73f2OJbmsrHH88PPlk3LIZNiwms95+O3zySdrVSpLS\nZvhQompqYMst4ec/h1degZdegh/+MD4ffDAMHep+M5JU7QwfKqpNN4WzzoLnnoPXX4ezz45Oqrn7\nzXz0UdqVSpJKxfChktloIzj9dPjzn2HmTPif/4G//S32mxk+PPabueaaWFUjSeq9DB9KxahRMHEi\nTJsG770Hl10WzctOPBFGjFgxmfXdd9OuVJKUNMOHUjdiRExQffDB2GV30iRYffWYKzJy5IrJrG++\nmXalkqQkGD5UVoYOhaOPht//Prqr3nRT9A856yzYcMPPT2aVJFUmw4fK1lprxcTU3/4WPvww9pv5\nyldirsimm8bHmWfCs8+634wkVRLDhypC7n4zU6bAmDFw+eUxGrLxxnDaabHfjEFEksqb4UMVZ9VV\nYb/9YPLkWBlz332w666x/8zWW8dkVvebkaTyZfhQRevfH/bYA669Ft5/H/7wBzjgAPjNb2LFzLrr\nxn4zDz7ofjOSVC4MH+o1+vaFnXeOZbvvvAOPPw5HHBE78e6+e+w30z6Z1f1mJCk9hg/1Sn36wDbb\nwPnnxxLdlpYYAXn88eiqOnz4isms7jcjSaVl+FCvV1MD9fXwn/8ZS3RffBFOPRVeeCH2mRk2bMVk\nVvebkaTiM3yoqtTUwGabxR4zzz8Pf/lLLNedOTN23h02LHbiveEG95uRpGIxfKiqbbwxnHEGPP00\nzJgB//3fMH9+zA2prYWxY91vRpKSZviQMtZbD045BR57LPabueSSWKp74onRZdX9ZiQpGYYPqQMj\nRsAJJ8BDD8V+M9deG43OsvebOf98eOuttCuVpMpj+JC6MHQofO97cM89MGcO/OpXEU7OPBM22GDF\nZNZXX027UkmqDIYPqQcGD47eIXfcEW3eb7sNNtkk5oqMHh2TWc86C557zjbvktQZw4eUp9VXh0MO\ngVtvjRGRKVOgoSHmhWyxRUxmPf10+POfDSKSlM3wISVgtdViv5kbb4wgcu+9sMsucP318PWvx2TW\niRNh2jT3m5GkfmkXIPU2/fvDnnvGx1VXReC4/fa4RXPJJbGE98ADo8FZQ0P0HpGkcrRwYXG+bzX8\n2KsHWlpaWqivr0+7FlWx5cvhyScjiNx+ezQ2k6Ty1go0kPmf1qS+qyMfUon06QPbbhsf558P06fH\nvjOSVK7efDPmriXN8CGloH2/GQfjJJWz1sTGOj7PCaeSJKmkDB+SJKmkDB+SJKmkDB+SJKmkDB+S\nJKmkDB+SJKmkDB+SJKmkDB+SJKmkDB+SJKmkDB+SJKmkDB+SJKmkDB+SJKmkDB+SJKmkDB+SJKmk\nyiV8nAC8BSwGngG27+L8nYCWzPlvAMcVtTr1WHNzc9olVB2veel5zUvPa947lEP4OBS4CDgX2AKY\nBtwLjOzk/PWBe4BHM+f/F3ApcFDRK1W3+QOi9Lzmpec1Lz2vee9QDuHjVGAScD3wGnAK8A5wfCfn\njwdmZF73GnBd5rU/LHahkiSpcGmHj/5APTA15/hUYNtOXrNNJ+dvBfRNtDpJkpS4tMPHUCIwzM45\nPgeo6+Q1tR2cPxvol/l+kiSpjPVLu4BSeeWVV9IuoaosWLCA1tbWtMuoKl7z0vOal57XvLSK9W9n\nTVG+a/f1BxYBBwNTso5fAnwV2KWD1zwKTAcmZh07EPh/wGrAspzzRwBPA+smU7IkSVXlPWAM8H5S\n3zDtkY9PiSWzY/l8+NgduKOT1zwB7JtzbCwRMHKDB8TFGkOEEEmS1DPvk2DwKBffBpYA44DRxLLb\nv7Niqe0mSfYNAAAG40lEQVR5wOSs8/8J+Bi4IHP+0ZnXH1iaciVJUm9wPNFk7B/ECEZ2k7FfAg/n\nnL8jMWLyD6LJ2PdLUKMkSZIkSZIkSZIkSZJKo6cb1qn7fkzMz/k70eTtDmCTDs47h1im9QnwB2DT\nEtVXDc4AlhOTtLOdg9c8SesCNwEfEq0BphOdmbOdg9c8KasQCw3eIq7nG8CZfLE1xDl4zfO1I3AX\ncf2WA/t3cM45rPz6DgAuA+YSi0CmYEsLIDasW0Kshvln4gf0QjrfsE49cy9wJLHi6KvEX+QZwMCs\nc04HFgAHAJsBzcRf5tVLWWgvNQZ4E3gWuDDruNc8WUOIv9fXEVs4jCL6D22QdY7XPFlnE/+gfZO4\n3t8ifsmZkHWO17wwewL/QVy/5cB+Oc935/peRezDtiuxyetDRDBPu3N66p4Crsg59jKxC66SN5T4\nS9w+ulRDrAv/UdY5/YH5uDqpUKsTmyruSvxG0h4+vObJ+2+isWFnvObJuwv4v5xjt7Oi5YLXPFm5\n4aM713ct4pf7Q7LOGQEsJfpudam3JpR8NqxTYQZnPn+U+bw+sQ9P9p/Bp8QPcv8MCnMFcDexBD17\nKNprnrz9iGX9vyZuL7YCx2Q97zVP3t3AbsDGmcdfA7YD7sk89poXV3eubwNxeyz7nPeBF+nmn0Ha\nHU6LJZ8N65S/GuK21jRidAlWXOeO/gxGlaiu3ugwYohzTOZxW9ZzXvPkbUD0IboA+Dnwr8ClxA/j\nG/GaF8M1RDPJ14jfpPsC/05soQFe82LrzvWtI/4/8Lecc2YTwaVLvTV8qLQuJ+4LdndCb1vXp6gD\nI4l9j3Yj/o8PEfy6s0eT1zw/fYA/Az/NPH4O2BwYT4SPlfGa52cCcBQRtF8CtgQuJn6z9pqnK7Hr\n21tvu3xI7POSm8Bq6YX96VN2GbAPMQlvVtbxDzKfO/oz+ADlowEYRgz9f5b52JH4Yf0pXvNimMWK\n0bx2r7LiN0CvefJ+ApwL3EaEj5uIkdUfZ573mhdXd67vB8T0hrVyzqmjm38GvTV8ZG9Yl2134PHS\nl9Mr1RAjHgcQEx9n5jz/FvGXMPvPoD+wE/4Z5OtB4rfur2U+tiCWkN+U+dprnrw/AV/JObYJsQIG\nvObFUMMXNwldzooRPq95cXXn+rYQv/xknzOCGAGv+j+DrjasU2GuJGY/70ik3faPVbPOOS1zzgHE\nP5q3AO8Cg0paae/2CJ/v8+E1T9ZWxC8zPwY2Ar5D9DRozDrHa56sa4klnHsRcz8OJOYbnJd1jte8\nMIOIX1i2IILdxMzX7f8+duf6Xgm8TfzyuSWx1LaV7t0G7vVWtmGdCrOc+O1kec7HkTnnnU0MXS/G\nRkDFkL3Utp3XPFl7A88T1/Ml4HsdnOM1T84g4HxWNBn7K9GTIneOotc8fzuz4md29s/x67PO6er6\n9icmX7c337PJmCRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJklTN+qZdgKSy9gixj8z9ZfgeM4hd\nNp9KuB5JRdZbN5aTlIw2ir9Neb7vUYraJBWB4UOSJJWU4UNSd/0b8AyxO/T7wM3AsKzndyY2pxoL\nTCc2BXsYqAX2AV4B/pZ53Wo533sV4HJiJ80PgXNznh8O3JX5nm8Ch3dQ36nEBnAfE7ttXoG7nEqS\nVHGyd80dB+xBbHP+deBx4PdZ5+5MhI8/AdsQW3T/BXiUmM/xNWJn6bnElt3tHiECzYXAxqzYtv6Y\nrHPuAZ7LvG898Bixk+aErHOagJ2AUcAuRNi5Ip//aEmSlJ7s8JFrDBE2BmYe75x5vEvWOadnjv1T\n1rGrgHuzHj8CvJjzvc8jtq8H2CTzPcZkPf/PmWMT6NwhRNCRVGa87SKpu7YEphCrTP5OBJM2YqQh\n2/NZX88hbpXMyDk2POtxG/Bkzvd4khgFqQFGA0uBp7Oefw1YkPOaXYAHgHcz9U0GvgSs2sV/l6QS\nM3xI6o6BwFTiH/XDga2AA4lw0D/n3M+yvm7Ledx+LPdnT02B9a1H3Jp5HjiIuDVzYif1SUpZv7QL\nkFQRvgKsDZwBvJc59q8Jfe8aYi5Htq2J+SJtwKvEz6oxrBj9+GdgcNb5WxGB5gdZxw5LqD5JCXPk\nQ9LKtI9IvA18Ssyx2ADYDzgzwfcZBVxAhIpG4CTgksxzrwH3Af9HBJ4GYBKwOOv1bxArZtrr+zfg\nuATrk5Qgw4eklWlv4vUhcBQxifMlYrXKD/hik6+Omn51dE5bzuPJxPLbp4DLgEuJsNFuHPAOsXLm\nN8A1xNyRds8SS21PB14gAsyPO6lHkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJkiRJ\nkiRJkiSpuv1/+1LYdYsgdQAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc680a5e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot the training and validation losses\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(dict_acc['lambda'],dict_acc['acc'])\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Network\n",
    "\n",
    "Now that we have determined the value of $\\lambda$ using the validation set, we can train the network one last time and predict on the test set. This will give us the prediction accuracy of our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.599484, Validation set accuracy: 0.964730\n",
      "Test set accuracy: 0.959596\n"
     ]
    }
   ],
   "source": [
    "# Best lambda\n",
    "lam_best = lambda_l2[np.argmax(dict_acc['acc'])]\n",
    "best_validation_acc = max(dict_acc['acc'])\n",
    "\n",
    "# One final training with the best model\n",
    "network, losses, acc = trainNetwork(epochs, learningRate, momentum, lam_best, n_hidden_1, n_hidden_2, K)\n",
    "\n",
    "## Predict on the test set\n",
    "A1, A2, A3, A4, P = network.forward(X_test)\n",
    "\n",
    "# Predict digits\n",
    "pred_test = np.argmax(P, axis = 0)\n",
    "        \n",
    "# Accuracy\n",
    "acc_test = np.sum((pred_test == y_test).astype(float))/y_test.shape[0]\n",
    "\n",
    "print \"Best lambda: %.6f, Validation set accuracy: %.6f\" % (lam_best, acc)\n",
    "print \"Test set accuracy: %.6f\" % acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good!\n",
    "\n",
    "Finally, let's plot the training and validation loss values during the training. Since we are using the full batch to train the model (i.e. all the training set), we do not observe any oscillations and a smooth decrease of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efc6637c790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAAFkCAYAAACn/timAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc663cf850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot the training and validation losses\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
